{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "with open('emotion_data.txt', 'rt', encoding='latin') as doc_file:\n",
    "    for line in doc_file: \n",
    "        documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "def get_emotion_cause_pairs(documents):\n",
    "    with open('emotion_seeds.pickle', 'rb') as f:\n",
    "        emotion_seeds = pickle.load(f)\n",
    "\n",
    "    document_clauses = []\n",
    "    emotion_labels = []\n",
    "    cause_labels = []\n",
    "    \n",
    "    emotion_count_dict = {}\n",
    "    emotion_cause_pairs = []\n",
    "    emotion_cause_pairs_label = []\n",
    "\n",
    "    for i, line in enumerate(documents):\n",
    "        emotion_of_line = \"\"\n",
    "        if line.startswith(\"<happy>\"):\n",
    "            emotion_of_line = \"happy\"\n",
    "        elif line.startswith(\"<sad>\"):\n",
    "            emotion_of_line = \"sad\"\n",
    "        elif line.startswith(\"<surprise>\"):\n",
    "            emotion_of_line = \"surprise\"\n",
    "        elif line.startswith(\"<disgust>\"):\n",
    "            emotion_of_line = \"disgust\"\n",
    "        elif line.startswith(\"<anger>\"):\n",
    "            emotion_of_line = \"anger\"\n",
    "        elif line.startswith(\"<shame>\"):\n",
    "            emotion_of_line = \"shame\"\n",
    "        elif line.startswith(\"<fear>\"):\n",
    "            emotion_of_line = \"fear\"\n",
    "\n",
    "        if emotion_of_line not in emotion_count_dict:\n",
    "            emotion_count_dict[emotion_of_line] = 1\n",
    "        else:\n",
    "            emotion_count_dict[emotion_of_line] += 1\n",
    "\n",
    "        clauses = re.split(r\"[.,!;:\\\"]+\", line)\n",
    "        emotion_clauses = []\n",
    "        cause_clauses = []\n",
    "        clauses_in_line = []\n",
    "        for clause in clauses:\n",
    "            cleaned_clause = remove_punctuation_from_clause(clause)\n",
    "            clause_words = cleaned_clause.split()\n",
    "\n",
    "            if not clause_words:\n",
    "                continue\n",
    "\n",
    "            document_clauses.append(clause_words)\n",
    "            clauses_in_line.append(clause_words)\n",
    "\n",
    "            if \"<cause>\" in clause:\n",
    "                cause_labels.append(1)\n",
    "                cause_clauses.append(clause_words)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "\n",
    "            has_seed = any(word.lower() in emotion_seeds for word in clause_words)\n",
    "            emotion_labels.append(1 if has_seed else 0)\n",
    "            if has_seed:\n",
    "                emotion_clauses.append(clause_words)\n",
    "\n",
    "        for m in range(len(clauses_in_line)):\n",
    "            for n in range(len(clauses_in_line)):\n",
    "                if clauses_in_line[m] in emotion_clauses and clauses_in_line[n] in cause_clauses:\n",
    "                    emotion_cause_pairs.append((clauses_in_line[m], clauses_in_line[n]))\n",
    "                    emotion_cause_pairs_label.append(emotion_of_line)\n",
    "                elif m != n:\n",
    "                    emotion_cause_pairs.append((clauses_in_line[m], clauses_in_line[n]))\n",
    "                    emotion_cause_pairs_label.append(\"None\")\n",
    "\n",
    "    # Use lists for mixed data types and return as-is\n",
    "    return (\n",
    "        document_clauses,\n",
    "        np.array(emotion_labels),\n",
    "        np.array(cause_labels),\n",
    "        emotion_cause_pairs,\n",
    "        emotion_cause_pairs_label,\n",
    "        emotion_count_dict\n",
    "    )\n",
    "\n",
    "def remove_punctuation_from_clause(clause):\n",
    "    clause = re.sub(r'<[^<]+>', \"\", clause)\n",
    "    clause = clause.translate(str.maketrans('', '', string.punctuation))\n",
    "    clause = clause.translate(str.maketrans('', '', string.digits))\n",
    "    return clause\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents in the dataset:  820\n",
      "Total pairs of clauses 5422\n",
      "Total pairs of emotion-cause clauses:  865\n",
      "\n",
      "Emotion wise count of sentences in the dataset \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>surprise</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>199</td>\n",
       "      <td>144</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   happy  sad  surprise  disgust  anger  fear  shame\n",
       "0    211  107        53       38    199   144     68"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_clauses, emotion_labels, cause_labels, emotion_cause_pairs,emotion_cause_pairs_labels,emotion_count_dict = get_emotion_cause_pairs(documents)\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Total number of documents in the dataset: \",len(documents))\n",
    "print(\"Total pairs of clauses\",len(emotion_cause_pairs))\n",
    "print(\"Total pairs of emotion-cause clauses: \",len(emotion_cause_pairs_labels)-emotion_cause_pairs_labels.count(\"None\"))\n",
    "print(\"\\nEmotion wise count of sentences in the dataset \\n\")\n",
    "pd.DataFrame(data = emotion_count_dict,index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(emotion_cause_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion wise count in train data {'None': 3280, 'happy': 161, 'anger': 151, 'fear': 113, 'disgust': 28, 'sad': 80, 'surprise': 40, 'shame': 50}\n",
      "Emotion wise count in validation data {'None': 365, 'happy': 18, 'fear': 12, 'anger': 17, 'shame': 6, 'disgust': 3, 'sad': 9, 'surprise': 4}\n",
      "Emotion wise count in test data {'None': 912, 'shame': 14, 'sad': 22, 'happy': 45, 'anger': 42, 'fear': 31, 'disgust': 8, 'surprise': 11}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "random.Random(56).shuffle(emotion_cause_pairs)\n",
    "random.Random(56).shuffle(emotion_cause_pairs_labels)\n",
    "\n",
    "#Split the emotion cause pairs into Train Data(72%), Validation Data(8%) and Test Data(20%)\n",
    "emotion_cause_pairs_inter, emotion_cause_pairs_test, emotion_cause_pairs_inter_labels, emotion_cause_pairs_test_labels = train_test_split(emotion_cause_pairs, emotion_cause_pairs_labels,\n",
    "                                                    stratify=emotion_cause_pairs_labels, \n",
    "                                                    test_size=0.20,random_state = 10)\n",
    "\n",
    "emotion_cause_pairs_train, emotion_cause_pairs_cv, emotion_cause_pairs_train_labels, emotion_cause_pairs_cv_labels = train_test_split(emotion_cause_pairs_inter, emotion_cause_pairs_inter_labels,\n",
    "                                                    stratify=emotion_cause_pairs_inter_labels, \n",
    "                                                    test_size=0.10,random_state = 10)\n",
    "\n",
    "\n",
    "emotion_count_dict_train = dict()\n",
    "for e in emotion_cause_pairs_train_labels:\n",
    "  if emotion_count_dict_train.get(e) == None:\n",
    "    emotion_count_dict_train[e] = 1\n",
    "  else:\n",
    "    emotion_count_dict_train[e]+=1\n",
    "print(\"Emotion wise count in train data\",emotion_count_dict_train)\n",
    "\n",
    "emotion_count_dict_cv = dict()\n",
    "for e in emotion_cause_pairs_cv_labels:\n",
    "  if emotion_count_dict_cv.get(e) == None:\n",
    "    emotion_count_dict_cv[e] = 1\n",
    "  else:\n",
    "    emotion_count_dict_cv[e]+=1\n",
    "print(\"Emotion wise count in validation data\",emotion_count_dict_cv)\n",
    "\n",
    "\n",
    "emotion_count_dict_test = dict()\n",
    "for e in emotion_cause_pairs_test_labels:\n",
    "  if emotion_count_dict_test.get(e) == None:\n",
    "    emotion_count_dict_test[e] = 1\n",
    "  else:\n",
    "    emotion_count_dict_test[e]+=1\n",
    "print(\"Emotion wise count in test data\",emotion_count_dict_test)\n",
    "\n",
    "emotion_cause_pairs_train_labels = [ 0 if i == \"None\" else 1 for i in emotion_cause_pairs_train_labels]\n",
    "emotion_cause_pairs_cv_labels = [ 0 if i == \"None\" else 1 for i in emotion_cause_pairs_cv_labels]\n",
    "emotion_cause_pairs_test_labels  = [ 0 if i == \"None\" else 1 for i in emotion_cause_pairs_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3903, 1085, 434)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_cause_pairs_train_labels),len(emotion_cause_pairs_test_labels),len(emotion_cause_pairs_cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################TRAIN DATA#################################################\n",
    "clauses_train = []\n",
    "emotion_train_labels = []\n",
    "cause_train_labels = []\n",
    "\n",
    "for i in range(len(emotion_cause_pairs_train)):\n",
    "  if emotion_cause_pairs_train_labels[i] == 1:\n",
    "    if emotion_cause_pairs_train[i][0] not in clauses_train:\n",
    "        clauses_train.append(emotion_cause_pairs_train[i][0])\n",
    "        emotion_train_labels.append(1)\n",
    "        cause_train_labels.append(0)\n",
    "    if emotion_cause_pairs_train[i][1] not in clauses_train:\n",
    "        clauses_train.append(emotion_cause_pairs_train[i][1])\n",
    "        emotion_train_labels.append(0)\n",
    "        cause_train_labels.append(1)\n",
    "\n",
    "clauses_cv = []\n",
    "emotion_cv_labels = []\n",
    "cause_cv_labels = []\n",
    "for i in range(len(emotion_cause_pairs_cv)):\n",
    "  if emotion_cause_pairs_cv_labels[i] == 1:\n",
    "    if emotion_cause_pairs_cv[i][0] not in clauses_cv:\n",
    "        clauses_cv.append(emotion_cause_pairs_cv[i][0])\n",
    "        emotion_cv_labels.append(1)\n",
    "        cause_cv_labels.append(0)\n",
    "    if emotion_cause_pairs_cv[i][1] not in clauses_cv:\n",
    "        clauses_cv.append(emotion_cause_pairs_cv[i][1])\n",
    "        emotion_cv_labels.append(0)\n",
    "        cause_cv_labels.append(1)\n",
    "\n",
    "\n",
    "clauses_test = []\n",
    "emotion_test_labels = []\n",
    "cause_test_labels = []\n",
    "for i in range(len(emotion_cause_pairs_test)):\n",
    "  if emotion_cause_pairs_test_labels[i] == 1:\n",
    "    if emotion_cause_pairs_test[i][0] not in clauses_test:\n",
    "        clauses_test.append(emotion_cause_pairs_test[i][0])\n",
    "        emotion_test_labels.append(1)\n",
    "        cause_test_labels.append(0)\n",
    "    if emotion_cause_pairs_test[i][1] not in clauses_test:\n",
    "        clauses_test.append(emotion_cause_pairs_test[i][1])\n",
    "        emotion_test_labels.append(0)\n",
    "        cause_test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3903, 623, 1085, 173, 434, 69)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_cause_pairs_train),emotion_cause_pairs_train_labels.count(1),len(emotion_cause_pairs_test),emotion_cause_pairs_test_labels.count(1),len(emotion_cause_pairs_cv),emotion_cause_pairs_cv_labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1166, 136, 333)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clauses_train),len(clauses_cv),len(clauses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1Score(emotion_cause_pairs_test,potential_emotion_cause_pairs_test,emotion_cause_pairs_test_labels,potential_emotion_cause_pairs_test_pred):\n",
    "  proposed_pairs = list(potential_emotion_cause_pairs_test_pred).count(1)\n",
    "  annotated_pairs = emotion_cause_pairs_test_labels.count(1)\n",
    "\n",
    "  #if both predicted and actual emotion cause pair are same then it is a correct pair\n",
    "  correct_pairs = 0\n",
    "  for i in range(len(emotion_cause_pairs_test)):\n",
    "    for j in range(potential_emotion_cause_pairs_test_pred.shape[0]):\n",
    "      if emotion_cause_pairs_test[i] == potential_emotion_cause_pairs_test[j]:\n",
    "        if emotion_cause_pairs_test_labels[i] == potential_emotion_cause_pairs_test_pred[j] and emotion_cause_pairs_test_labels[i] == 1:\n",
    "          correct_pairs+=1\n",
    "\n",
    "  precision = correct_pairs/proposed_pairs if proposed_pairs > 0  else 0\n",
    "  recall = correct_pairs/annotated_pairs\n",
    "\n",
    "  F1_score = 0\n",
    "  if precision+recall !=0:\n",
    "    F1_score = 2 * precision * recall/(precision+recall)\n",
    "\n",
    "  print(\"Correct pairs found: {} Proposed pairs: {} Annotated Pairs: {}  \".format(correct_pairs,proposed_pairs,annotated_pairs))\n",
    "\n",
    "  print(\"Precision:{}  Recall:{}   F1-Score:{}\".format(precision,recall,F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302 333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "clauses_train.extend(clauses_cv)\n",
    "emotion_train_labels.extend(emotion_cv_labels)\n",
    "cause_train_labels.extend(cause_cv_labels)\n",
    "\n",
    "X_train_text = [\" \".join(l) for l in clauses_train]\n",
    "X_test_text =  [\" \".join(l) for l in clauses_test]\n",
    "print(len(X_train_text),len(X_test_text))\n",
    "tfidfVectorizer = TfidfVectorizer(min_df = 0)\n",
    "X_train_vectors = tfidfVectorizer.fit_transform(X_train_text)\n",
    "X_test_vectors =  tfidfVectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>0.909326</td>\n",
       "      <td>0.999360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.921653</td>\n",
       "      <td>0.999360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>0.998848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.917030</td>\n",
       "      <td>0.966592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888610</td>\n",
       "      <td>0.924473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.842111</td>\n",
       "      <td>0.876430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654890</td>\n",
       "      <td>0.666304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>0.323143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>0.323143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_alpha  mean_test_score  mean_train_score\n",
       "0       1e-06         0.909326          0.999360\n",
       "1       1e-05         0.921653          0.999360\n",
       "2      0.0001         0.930100          0.998848\n",
       "3       0.001         0.917030          0.966592\n",
       "4        0.01         0.888610          0.924473\n",
       "5         0.1         0.842111          0.876430\n",
       "6           1         0.654890          0.666304\n",
       "7          10         0.323144          0.323143\n",
       "8         100         0.323144          0.323143"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "logistic_classifier = SGDClassifier(loss = 'log',random_state = 0,n_jobs = -1)\n",
    "param_grid = {'alpha':[10**-6,10**(-5),10**(-4),10**(-3),10**(-2),10**(-1),1,10,100]}\n",
    "gridSearch = GridSearchCV(logistic_classifier, param_grid,scoring = 'f1_weighted',cv = 7)\n",
    "gridSearch.return_train_score = True\n",
    "gridSearch.fit(X_train_vectors,emotion_train_labels)\n",
    "\n",
    "dataframe = pd.DataFrame(gridSearch.cv_results_)\n",
    "dataframe[['param_alpha','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>micro avg</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.929227</td>\n",
       "      <td>0.930326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.929041</td>\n",
       "      <td>0.927928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.928144</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.927927</td>\n",
       "      <td>0.927920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>support</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1   micro avg   macro avg  weighted avg\n",
       "precision    0.895954    0.962500    0.927928    0.929227      0.930326\n",
       "recall       0.962733    0.895349    0.927928    0.929041      0.927928\n",
       "f1-score     0.928144    0.927711    0.927928    0.927927      0.927920\n",
       "support    161.000000  172.000000  333.000000  333.000000    333.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  classification_report\n",
    "\n",
    "emotion_pred_labels = gridSearch.best_estimator_.predict(X_test_vectors)\n",
    "class_report = classification_report(emotion_test_labels, emotion_pred_labels, target_names=[0,1], output_dict=True)\n",
    "report = pd.DataFrame(data = class_report)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\yuming\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>0.917787</td>\n",
       "      <td>0.999424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.925496</td>\n",
       "      <td>0.999424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.929334</td>\n",
       "      <td>0.998847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.917811</td>\n",
       "      <td>0.967551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.888519</td>\n",
       "      <td>0.925482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.830669</td>\n",
       "      <td>0.874115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.798126</td>\n",
       "      <td>0.838977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.339358</td>\n",
       "      <td>0.339576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.339358</td>\n",
       "      <td>0.339576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_alpha  mean_test_score  mean_train_score\n",
       "0       1e-06         0.917787          0.999424\n",
       "1       1e-05         0.925496          0.999424\n",
       "2      0.0001         0.929334          0.998847\n",
       "3       0.001         0.917811          0.967551\n",
       "4        0.01         0.888519          0.925482\n",
       "5         0.1         0.830669          0.874115\n",
       "6           1         0.798126          0.838977\n",
       "7          10         0.339358          0.339576\n",
       "8         100         0.339358          0.339576"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logistic_classifier = SGDClassifier(loss = 'log',random_state = 0,n_jobs = 2)\n",
    "param_grid = {'alpha':[10**-6,10**(-5),10**(-4),10**(-3),10**(-2),10**(-1),1,10,100]}\n",
    "gridSearch = GridSearchCV(logistic_classifier, param_grid,scoring = 'f1_weighted',cv = 5)\n",
    "gridSearch.return_train_score = True\n",
    "gridSearch.fit(X_train_vectors,cause_train_labels)\n",
    "\n",
    "dataframe = pd.DataFrame(gridSearch.cv_results_)\n",
    "dataframe[['param_alpha','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non-emotion-cause</th>\n",
       "      <th>emotion-cause</th>\n",
       "      <th>micro avg</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.929227</td>\n",
       "      <td>0.930326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.929041</td>\n",
       "      <td>0.927928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.928144</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.927927</td>\n",
       "      <td>0.927920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>support</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           non-emotion-cause  emotion-cause   micro avg   macro avg  \\\n",
       "precision           0.962500       0.895954    0.927928    0.929227   \n",
       "recall              0.895349       0.962733    0.927928    0.929041   \n",
       "f1-score            0.927711       0.928144    0.927928    0.927927   \n",
       "support           172.000000     161.000000  333.000000  333.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.930326  \n",
       "recall         0.927928  \n",
       "f1-score       0.927920  \n",
       "support      333.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  classification_report\n",
    "\n",
    "cause_pred_labels = gridSearch.best_estimator_.predict(X_test_vectors)\n",
    "class_report = classification_report(cause_test_labels, cause_pred_labels, target_names=[\"non-emotion-cause\",\"emotion-cause\"], output_dict=True)\n",
    "report = pd.DataFrame(data = class_report)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
